{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KBDatalakeApps Pipeline Step-by-Step Testing\n",
    "\n",
    "This notebook is a **test harness** for `KBDataLakeUtils` from\n",
    "`lib/KBDatalakeApps/KBDatalakeUtils.py`. It mirrors how the KBase SDK impl file\n",
    "works: the notebook environment reads token/config from files, then creates\n",
    "a separate `KBDataLakeUtils` instance with those credentials injected.\n",
    "\n",
    "**Architecture:**\n",
    "- `util.py` (NotebookUtil) = test harness that reads token/config from files\n",
    "- `KBDataLakeUtils` = the code under test, receives token/config as arguments\n",
    "- Each cell creates a fresh `KBDataLakeUtils` instance (state lives on filesystem)\n",
    "\n",
    "**Pipeline steps tested:**\n",
    "1. Process input arguments into user genome table\n",
    "2. Download genome assemblies\n",
    "3. Download genome genes & annotations\n",
    "4. Run SKANI analysis\n",
    "5. Annotate genomes with RAST\n",
    "6. Build metabolic models (single + parallel)\n",
    "7. Run phenotype simulations\n",
    "8. Build SQLite database\n",
    "9. Save annotated genomes to KBase\n",
    "10. Save models to KBase\n",
    "11. Generate KBase report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup & Configuration\n",
    "\n",
    "Configure the pipeline parameters and save them to datacache. Every subsequent\n",
    "cell reloads this config and creates a fresh `KBDataLakeUtils` instance.\n",
    "\n",
    "The notebook's `util` reads token/config from your standard files\n",
    "(`~/.tokens`, `~/.kbase/token`, `~/.kbutillib/config.yaml`). The\n",
    "`create_pipeline_utils()` helper then injects those into `KBDataLakeUtils`\n",
    "while blocking it from reading files itself.\n",
    "\n",
    "- **Edit**: `workspace_name`, `input_refs`, and optionally `pipeline_dir`\n",
    "- **Output**: `pipeline_config` saved to datacache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run util.py\n",
    "import os\n",
    "\n",
    "# ---- EDIT THESE PARAMETERS FOR YOUR TEST ----\n",
    "workspace_name = 'chenry:narrative_1234567890'  # Your KBase workspace\n",
    "input_refs = [\n",
    "    # Add genome or genome set references here, e.g.:\n",
    "    # '12345/6/7',\n",
    "    # '12345/8/1',\n",
    "]\n",
    "worker_count = 4\n",
    "pipeline_dir = os.path.join(util.data_dir, 'pipeline_test')\n",
    "# ---- END PARAMETERS ----\n",
    "\n",
    "parameters = {\n",
    "    'input_refs': input_refs,\n",
    "    'workspace_name': workspace_name,\n",
    "    'suffix': '.datalake',\n",
    "}\n",
    "\n",
    "# Verify the notebook util has a token loaded\n",
    "token = util.get_token('kbase')\n",
    "print(f'KBase token loaded: {\"yes\" if token else \"NO - check ~/.kbase/token or ~/.tokens\"}')\n",
    "print(f'Config keys: {list(util._config_hash.keys()) if util._config_hash else \"none\"}')\n",
    "\n",
    "# Save configuration so other cells can reload it\n",
    "util.save('pipeline_config', {\n",
    "    'workspace_name': workspace_name,\n",
    "    'input_refs': input_refs,\n",
    "    'parameters': parameters,\n",
    "    'pipeline_dir': pipeline_dir,\n",
    "    'worker_count': worker_count,\n",
    "})\n",
    "\n",
    "# Quick smoke test: create a pipeline utils instance\n",
    "os.makedirs(pipeline_dir, exist_ok=True)\n",
    "pipeline = util.create_pipeline_utils(\n",
    "    directory=pipeline_dir,\n",
    "    workspace_name=workspace_name,\n",
    "    parameters=parameters,\n",
    "    worker_count=worker_count,\n",
    ")\n",
    "print(f'Pipeline directory: {pipeline.directory}')\n",
    "print(f'Pipeline workspace: {pipeline.workspace_name}')\n",
    "print(f'Pipeline token loaded: {\"yes\" if pipeline.get_token(\"kbase\") else \"NO\"}')\n",
    "print(f'\\nSetup complete. Ready to test pipeline steps.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Process Arguments into User Genome Table\n",
    "\n",
    "Tests `pipeline_process_arguments_into_user_genome_table()`.\n",
    "\n",
    "Translates the input reference list (genomes or genome sets) into a structured\n",
    "metadata table saved as `user_genomes.tsv`.\n",
    "\n",
    "- **Input**: `input_refs` from KBase workspace\n",
    "- **Output**: `<pipeline_dir>/user_genomes.tsv`\n",
    "- **Columns**: genome_id, species_name, taxonomy, genome_ref, assembly_ref,\n",
    "  genome_type, genome_source_id, genome_source_name, num_contigs, num_proteins,\n",
    "  num_noncoding_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run util.py\n",
    "import os\n",
    "\n",
    "config = util.load('pipeline_config')\n",
    "pipeline = util.create_pipeline_utils(\n",
    "    directory=config['pipeline_dir'],\n",
    "    workspace_name=config['workspace_name'],\n",
    "    parameters=config['parameters'],\n",
    "    worker_count=config['worker_count'],\n",
    ")\n",
    "\n",
    "# Run the actual pipeline method\n",
    "pipeline.pipeline_process_arguments_into_user_genome_table()\n",
    "\n",
    "# Inspect the output\n",
    "output_path = os.path.join(config['pipeline_dir'], 'user_genomes.tsv')\n",
    "if os.path.exists(output_path):\n",
    "    df = pd.read_csv(output_path, sep='\\t')\n",
    "    print(f'\\nGenomes table: {len(df)} rows, {len(df.columns)} columns')\n",
    "    display(df)\n",
    "else:\n",
    "    print(f'Output file not created: {output_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download Genome Assemblies\n",
    "\n",
    "Tests `pipeline_download_user_genome_assmemblies()`.\n",
    "\n",
    "Downloads FASTA assembly files for all genomes listed in `user_genomes.tsv`.\n",
    "\n",
    "- **Input**: `user_genomes.tsv` (assembly_ref column)\n",
    "- **Output**: FASTA files in `<pipeline_dir>/assemblies/`\n",
    "- **Requires**: KBase workspace access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run util.py\n",
    "import os\n",
    "\n",
    "config = util.load('pipeline_config')\n",
    "pipeline = util.create_pipeline_utils(\n",
    "    directory=config['pipeline_dir'],\n",
    "    workspace_name=config['workspace_name'],\n",
    "    parameters=config['parameters'],\n",
    "    worker_count=config['worker_count'],\n",
    ")\n",
    "\n",
    "# Run the actual pipeline method\n",
    "pipeline.pipeline_download_user_genome_assmemblies()\n",
    "\n",
    "# Inspect output\n",
    "assemblies_dir = os.path.join(config['pipeline_dir'], 'assemblies')\n",
    "if os.path.exists(assemblies_dir):\n",
    "    files = os.listdir(assemblies_dir)\n",
    "    print(f'\\nAssembly files ({len(files)}):')\n",
    "    for f in sorted(files):\n",
    "        size_kb = os.path.getsize(os.path.join(assemblies_dir, f)) / 1024\n",
    "        print(f'  {f}: {size_kb:.1f} KB')\n",
    "else:\n",
    "    print('No assemblies directory created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download Genome Genes & Annotations\n",
    "\n",
    "Tests `pipeline_download_user_genome_genes()`.\n",
    "\n",
    "Downloads genes, features, and existing annotations for each genome into\n",
    "per-genome TSV files.\n",
    "\n",
    "- **Input**: `user_genomes.tsv` (genome_ref column)\n",
    "- **Output**: `<pipeline_dir>/genomes/<genome_id>.tsv` per genome\n",
    "- **Columns**: gene_id, aliases, contig, start, end, strand, type, functions,\n",
    "  protein_translation, dna_sequence, ontology_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run util.py\n",
    "import os\n",
    "\n",
    "config = util.load('pipeline_config')\n",
    "pipeline = util.create_pipeline_utils(\n",
    "    directory=config['pipeline_dir'],\n",
    "    workspace_name=config['workspace_name'],\n",
    "    parameters=config['parameters'],\n",
    "    worker_count=config['worker_count'],\n",
    ")\n",
    "\n",
    "# Run the actual pipeline method\n",
    "pipeline.pipeline_download_user_genome_genes()\n",
    "\n",
    "# Inspect output\n",
    "genomes_dir = os.path.join(config['pipeline_dir'], 'genomes')\n",
    "if os.path.exists(genomes_dir):\n",
    "    files = [f for f in os.listdir(genomes_dir) if f.endswith('.tsv')]\n",
    "    print(f'\\nGenome gene files ({len(files)}):')\n",
    "    for f in sorted(files):\n",
    "        df = pd.read_csv(os.path.join(genomes_dir, f), sep='\\t')\n",
    "        print(f'  {f}: {len(df)} features')\n",
    "    # Show sample from first genome\n",
    "    if files:\n",
    "        sample = pd.read_csv(os.path.join(genomes_dir, files[0]), sep='\\t')\n",
    "        print(f'\\nSample from {files[0]}:')\n",
    "        display(sample.head())\n",
    "else:\n",
    "    print('No genomes directory created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run SKANI Analysis\n",
    "\n",
    "Tests `pipeline_run_skani_analysis()`.\n",
    "\n",
    "Runs SKANI (fast genomic distance estimation) against three sketch databases:\n",
    "pangenome, fitness, and phenotype.\n",
    "\n",
    "- **Input**: FASTA files in `<pipeline_dir>/assemblies/`\n",
    "- **Output**: TSV files in `<pipeline_dir>/skani/` (one per database)\n",
    "- **Columns**: genome_id, reference_genome, ani_percentage\n",
    "- **Requires**: SKANI sketch databases configured in kbutillib config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run util.py\n",
    "import os\n",
    "\n",
    "config = util.load('pipeline_config')\n",
    "pipeline = util.create_pipeline_utils(\n",
    "    directory=config['pipeline_dir'],\n",
    "    workspace_name=config['workspace_name'],\n",
    "    parameters=config['parameters'],\n",
    "    worker_count=config['worker_count'],\n",
    ")\n",
    "\n",
    "# Run the actual pipeline method\n",
    "pipeline.pipeline_run_skani_analysis()\n",
    "\n",
    "# Inspect output\n",
    "skani_dir = os.path.join(config['pipeline_dir'], 'skani')\n",
    "if os.path.exists(skani_dir):\n",
    "    files = [f for f in os.listdir(skani_dir) if f.endswith('.tsv')]\n",
    "    print(f'\\nSKANI result files ({len(files)}):')\n",
    "    for f in sorted(files):\n",
    "        df = pd.read_csv(os.path.join(skani_dir, f), sep='\\t')\n",
    "        print(f'  {f}: {len(df)} hits')\n",
    "        display(df.head())\n",
    "else:\n",
    "    print('No skani directory created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Annotate Genomes with RAST\n\nTests `pipeline_annotate_user_genome_with_rast()`.\n\nSubmits protein sequences from each genome to RAST for functional annotation.\nTranslates RAST functions to SSO (Subsystem Ontology) terms and populates the\n`Annotation:SSO` column in each genome TSV file. Skips genomes that already\nhave `Annotation:SSO` data (e.g., from the original KBase genome object).\n\n- **Input**: Genome TSV files in `<pipeline_dir>/genomes/`\n- **Output**: Updated genome TSV files with `Annotation:SSO` column\n- **Format**: `SSO:nnnnn:description|rxn1,rxn2` entries separated by `;`\n- **Requires**: RAST SDK service access"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%run util.py\nimport os\n\nconfig = util.load('pipeline_config')\npipeline = util.create_pipeline_utils(\n    directory=config['pipeline_dir'],\n    parameters=config['parameters'],\n    kb_version=config.get('kb_version', 'dev'),\n    worker_count=config['worker_count'],\n)\n\n# Run the actual pipeline method\npipeline.pipeline_annotate_user_genome_with_rast()\n\n# Inspect output - check that Annotation:SSO column was added/populated\ngenomes_dir = os.path.join(config['pipeline_dir'], 'genomes')\nif os.path.exists(genomes_dir):\n    files = [f for f in os.listdir(genomes_dir) if f.endswith('.tsv')]\n    for f in sorted(files):\n        df = pd.read_csv(os.path.join(genomes_dir, f), sep='\\t')\n        has_sso = 'Annotation:SSO' in df.columns\n        annotated = df['Annotation:SSO'].fillna('').astype(str).str.strip().ne('').sum() if has_sso else 0\n        print(f'{f}: {len(df)} features, Annotation:SSO={has_sso}, annotated={annotated}')\n    # Show sample\n    if files:\n        sample = pd.read_csv(os.path.join(genomes_dir, files[0]), sep='\\t')\n        if 'Annotation:SSO' in sample.columns:\n            print(f'\\nSample annotations from {files[0]}:')\n            display(sample[['gene_id', 'functions', 'Annotation:SSO']].head(10))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6a: Build Single Metabolic Model (Debug Mode)\n\nTests model building for **one** genome in the current process (no parallelism).\nThis calls the same core logic as `pipeline_run_moddeling_analysis()` but is\neasier to debug. We manually replicate the worker logic here to allow\nstep-through inspection.\n\n- **Input**: Genome TSV with `Annotation:SSO` column\n- **Output**: COBRA JSON model in `<pipeline_dir>/models/<genome_id>_model.json`\n- **Note**: Edit `test_genome_id` to pick which genome to test"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%run util.py\nimport os\nimport cobra\nfrom modelseedpy.core.msgenome import MSGenome, MSFeature\nfrom modelseedpy import MSModelUtil\n\nconfig = util.load('pipeline_config')\npipeline = util.create_pipeline_utils(\n    directory=config['pipeline_dir'],\n    parameters=config['parameters'],\n    kb_version=config.get('kb_version', 'dev'),\n    worker_count=config['worker_count'],\n)\n\n# Pick a genome to test\ngenomes_file = os.path.join(config['pipeline_dir'], 'user_genomes.tsv')\nuser_genomes = pd.read_csv(genomes_file, sep='\\t')\ntest_genome_id = user_genomes.iloc[0]['genome_id']  # change index to pick another\nprint(f'Building model for: {test_genome_id}')\nprint('=' * 60)\n\n# Load features from genome TSV\ngenome_tsv = os.path.join(config['pipeline_dir'], 'genomes', f'{test_genome_id}.tsv')\ngene_df = pd.read_csv(genome_tsv, sep='\\t')\n\nsafe_id = test_genome_id.replace('.', '_')\nmodels_dir = os.path.join(config['pipeline_dir'], 'models')\nos.makedirs(models_dir, exist_ok=True)\n\n# Create MSGenome from features\ngenome = MSGenome()\ngenome.id = safe_id\ngenome.scientific_name = test_genome_id\n\nms_features = []\nfor _, gene in gene_df.iterrows():\n    protein = gene.get('protein_translation', '')\n    gene_id = gene.get('gene_id', '')\n    if pd.notna(protein) and protein:\n        feature = MSFeature(gene_id, str(protein))\n        # Parse Annotation:SSO column\n        # Format: SSO:nnnnn:description|rxn1,rxn2;SSO:mmmmm:desc2|rxn3\n        sso_col = gene.get('Annotation:SSO', '')\n        if pd.notna(sso_col) and sso_col:\n            for entry in str(sso_col).split(';'):\n                entry = entry.strip()\n                if not entry:\n                    continue\n                term_part = entry.split('|')[0]\n                parts = term_part.split(':')\n                if len(parts) >= 2 and parts[0] == 'SSO':\n                    sso_id = parts[0] + ':' + parts[1]\n                    feature.add_ontology_term('SSO', sso_id)\n                    # Extract description for classifier\n                    if len(parts) >= 3:\n                        description = ':'.join(parts[2:])\n                        if description:\n                            feature.add_ontology_term('RAST', description)\n        ms_features.append(feature)\n\ngenome.add_features(ms_features)\nprint(f'MSGenome: {len(ms_features)} protein features')\n\n# Build the model using pipeline's reconstruction utils\ngenome_classifier = pipeline.get_classifier()\nbuild_output, mdlutl = pipeline.build_metabolic_model(\n    genome=genome,\n    genome_classifier=genome_classifier,\n    model_id=safe_id,\n    model_name=test_genome_id,\n    gs_template='auto',\n    atp_safe=True,\n    load_default_medias=True,\n    max_gapfilling=10,\n    gapfilling_delta=0,\n)\n\nif mdlutl is not None:\n    model = mdlutl.model\n    print(f'\\nModel built: {model.id}')\n    print(f'  Reactions: {len(model.reactions)}')\n    print(f'  Metabolites: {len(model.metabolites)}')\n    print(f'  Genes: {len(model.genes)}')\n    print(f'  Class: {build_output.get(\"Class\", \"N/A\")}')\n    print(f'  Core GF: {build_output.get(\"Core GF\", 0)}')\n\n    # Gapfill on Carbon-Pyruvic-Acid\n    gapfill_media = pipeline.get_media('KBaseMedia/Carbon-Pyruvic-Acid')\n    gf_output, _, _, _ = pipeline.gapfill_metabolic_model(\n        mdlutl=mdlutl,\n        genome=genome,\n        media_objs=[gapfill_media],\n        templates=[model.template],\n        atp_safe=True,\n        objective='bio1',\n        minimum_objective=0.01,\n        gapfilling_mode='Sequential',\n    )\n    print(f'  GS GF: {gf_output.get(\"GS GF\", 0)}')\n    print(f'  Growth: {gf_output.get(\"Growth\", \"N/A\")}')\n\n    # Save model\n    model_path = os.path.join(models_dir, f'{safe_id}_model.json')\n    cobra.io.save_json_model(model, model_path)\n    print(f'  Saved: {model_path}')\nelse:\n    print(f'\\nModel build returned None: {build_output}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6b: Build All Metabolic Models (Parallel)\n\nTests `pipeline_run_moddeling_analysis()`.\n\nBuilds metabolic models for **all** genomes using ProcessPoolExecutor,\nmatching the production pipeline behavior. Uses `Annotation:SSO` column\nfrom genome TSV files as the source of functional annotations.\n\n- **Input**: All genome TSV files in `<pipeline_dir>/genomes/` with `Annotation:SSO`\n- **Output**: COBRA JSON models in `<pipeline_dir>/models/`\n- **Uses**: `worker_count` parallel processes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run util.py\n",
    "import os\n",
    "\n",
    "config = util.load('pipeline_config')\n",
    "pipeline = util.create_pipeline_utils(\n",
    "    directory=config['pipeline_dir'],\n",
    "    workspace_name=config['workspace_name'],\n",
    "    parameters=config['parameters'],\n",
    "    worker_count=config['worker_count'],\n",
    ")\n",
    "\n",
    "# Run the actual pipeline method\n",
    "pipeline.pipeline_run_moddeling_analysis()\n",
    "\n",
    "# Inspect output\n",
    "import cobra\n",
    "models_dir = os.path.join(config['pipeline_dir'], 'models')\n",
    "if os.path.exists(models_dir):\n",
    "    model_files = [f for f in os.listdir(models_dir) if f.endswith('_model.json')]\n",
    "    print(f'\\nModels built ({len(model_files)}):')\n",
    "    for mf in sorted(model_files):\n",
    "        model = cobra.io.load_json_model(os.path.join(models_dir, mf))\n",
    "        print(f'  {mf}: {len(model.reactions)} rxns, {len(model.metabolites)} mets, {len(model.genes)} genes')\n",
    "else:\n",
    "    print('No models directory created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Run Phenotype Simulations\n",
    "\n",
    "Tests `pipeline_run_phenotype_simulations()`.\n",
    "\n",
    "Runs phenotype simulations for all built models using ProcessPoolExecutor.\n",
    "Also builds summary tables from the individual simulation results.\n",
    "\n",
    "- **Input**: COBRA JSON models in `<pipeline_dir>/models/`\n",
    "- **Output**: Per-model JSON in `<pipeline_dir>/phenotypes/` and summary TSV tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run util.py\n",
    "import os\n",
    "\n",
    "config = util.load('pipeline_config')\n",
    "pipeline = util.create_pipeline_utils(\n",
    "    directory=config['pipeline_dir'],\n",
    "    workspace_name=config['workspace_name'],\n",
    "    parameters=config['parameters'],\n",
    "    worker_count=config['worker_count'],\n",
    ")\n",
    "\n",
    "# Run the actual pipeline method\n",
    "pipeline.pipeline_run_phenotype_simulations()\n",
    "\n",
    "# Inspect output\n",
    "phenotypes_dir = os.path.join(config['pipeline_dir'], 'phenotypes')\n",
    "if os.path.exists(phenotypes_dir):\n",
    "    files = os.listdir(phenotypes_dir)\n",
    "    json_count = len([f for f in files if f.endswith('.json')])\n",
    "    tsv_count = len([f for f in files if f.endswith('.tsv')])\n",
    "    print(f'\\nPhenotype results: {json_count} simulation JSONs, {tsv_count} summary TSVs')\n",
    "    for f in sorted(files):\n",
    "        if f.endswith('.tsv'):\n",
    "            df = pd.read_csv(os.path.join(phenotypes_dir, f), sep='\\t')\n",
    "            print(f'\\n  {f}: {len(df)} rows')\n",
    "            display(df.head())\n",
    "else:\n",
    "    print('No phenotypes directory created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Build SQLite Database\n",
    "\n",
    "Tests `pipeline_build_sqllite_db()`.\n",
    "\n",
    "Compiles all output data into a single SQLite database.\n",
    "\n",
    "- **Input**: All TSV output files from previous pipeline steps\n",
    "- **Output**: `<pipeline_dir>/berdl_tables.db` with tables:\n",
    "  genome, genome_ani, genome_features, genome_accuracy,\n",
    "  genome_gene_phenotype_reactions, genome_phenotype_gaps, gapfilled_reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run util.py\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "config = util.load('pipeline_config')\n",
    "pipeline = util.create_pipeline_utils(\n",
    "    directory=config['pipeline_dir'],\n",
    "    workspace_name=config['workspace_name'],\n",
    "    parameters=config['parameters'],\n",
    "    worker_count=config['worker_count'],\n",
    ")\n",
    "\n",
    "# Run the actual pipeline method\n",
    "pipeline.pipeline_build_sqllite_db()\n",
    "\n",
    "# Inspect the database\n",
    "db_path = os.path.join(config['pipeline_dir'], 'berdl_tables.db')\n",
    "if os.path.exists(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "    print(f'\\nSQLite database: {db_path}')\n",
    "    print(f'Tables ({len(tables)}):')\n",
    "    for table in tables:\n",
    "        cursor.execute(f'SELECT COUNT(*) FROM [{table}]')\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f'  {table}: {count} rows')\n",
    "        sample = pd.read_sql_query(f'SELECT * FROM [{table}] LIMIT 3', conn)\n",
    "        display(sample)\n",
    "    conn.close()\n",
    "else:\n",
    "    print(f'Database not found: {db_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 9: Save Annotated Genomes to KBase\n\nTests `pipeline_save_annotated_genomes()`.\n\nSaves RAST-annotated genomes back to the KBase workspace and creates a GenomeSet.\n\n- **Input**: Genome TSV files with `Annotation:SSO`, user_genomes.tsv for refs\n- **Output**: New genome objects + GenomeSet in KBase workspace\n- **Warning**: This writes to the KBase workspace - use a test workspace"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run util.py\n",
    "\n",
    "config = util.load('pipeline_config')\n",
    "pipeline = util.create_pipeline_utils(\n",
    "    directory=config['pipeline_dir'],\n",
    "    workspace_name=config['workspace_name'],\n",
    "    parameters=config['parameters'],\n",
    "    worker_count=config['worker_count'],\n",
    ")\n",
    "\n",
    "# Run the actual pipeline method\n",
    "pipeline.pipeline_save_annotated_genomes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save Models to KBase\n",
    "\n",
    "Tests `pipeline_save_models_to_kbase()`.\n",
    "\n",
    "Saves built COBRA metabolic models to the KBase workspace.\n",
    "\n",
    "- **Input**: COBRA JSON models in `<pipeline_dir>/models/`, user_genomes.tsv\n",
    "- **Output**: FBAModel objects in KBase workspace\n",
    "- **Warning**: This writes to the KBase workspace - use a test workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run util.py\n",
    "\n",
    "config = util.load('pipeline_config')\n",
    "pipeline = util.create_pipeline_utils(\n",
    "    directory=config['pipeline_dir'],\n",
    "    workspace_name=config['workspace_name'],\n",
    "    parameters=config['parameters'],\n",
    "    worker_count=config['worker_count'],\n",
    ")\n",
    "\n",
    "# Run the actual pipeline method\n",
    "pipeline.pipeline_save_models_to_kbase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Generate KBase Report\n",
    "\n",
    "Tests `pipeline_save_kbase_report()`.\n",
    "\n",
    "Generates an HTML report summarizing pipeline results and saves it to KBase.\n",
    "\n",
    "- **Input**: All pipeline output data (genomes, models, SQLite DB)\n",
    "- **Output**: KBase report object with HTML viewer and downloadable SQLite DB\n",
    "- **Warning**: This writes to the KBase workspace - use a test workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run util.py\n",
    "\n",
    "config = util.load('pipeline_config')\n",
    "pipeline = util.create_pipeline_utils(\n",
    "    directory=config['pipeline_dir'],\n",
    "    workspace_name=config['workspace_name'],\n",
    "    parameters=config['parameters'],\n",
    "    worker_count=config['worker_count'],\n",
    ")\n",
    "\n",
    "# Run the actual pipeline method\n",
    "pipeline.pipeline_save_kbase_report()\n",
    "\n",
    "if hasattr(pipeline, 'report_name'):\n",
    "    print(f'\\nReport: {pipeline.report_name} ({pipeline.report_ref})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection: Review All Pipeline Outputs\n",
    "\n",
    "Comprehensive view of all outputs across all pipeline steps.\n",
    "No `KBDataLakeUtils` needed - just reads the filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run util.py\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "config = util.load('pipeline_config')\n",
    "pipeline_dir = config['pipeline_dir']\n",
    "\n",
    "print('=' * 60)\n",
    "print('PIPELINE OUTPUT SUMMARY')\n",
    "print('=' * 60)\n",
    "\n",
    "dirs_to_check = [\n",
    "    ('user_genomes.tsv', 'User Genomes Table'),\n",
    "    ('assemblies', 'Assemblies'),\n",
    "    ('genomes', 'Genome Gene Tables'),\n",
    "    ('skani', 'SKANI Results'),\n",
    "    ('models', 'Metabolic Models'),\n",
    "    ('phenotypes', 'Phenotype Simulations'),\n",
    "    ('berdl_tables.db', 'SQLite Database'),\n",
    "]\n",
    "\n",
    "for item, label in dirs_to_check:\n",
    "    full_path = os.path.join(pipeline_dir, item)\n",
    "    if os.path.isfile(full_path):\n",
    "        size_kb = os.path.getsize(full_path) / 1024\n",
    "        print(f'\\n{label}: {full_path} ({size_kb:.1f} KB)')\n",
    "        if full_path.endswith('.tsv'):\n",
    "            df = pd.read_csv(full_path, sep='\\t')\n",
    "            print(f'  Rows: {len(df)}, Columns: {list(df.columns)}')\n",
    "    elif os.path.isdir(full_path):\n",
    "        files = os.listdir(full_path)\n",
    "        print(f'\\n{label}: {full_path} ({len(files)} files)')\n",
    "        for f in sorted(files):\n",
    "            fp = os.path.join(full_path, f)\n",
    "            size_kb = os.path.getsize(fp) / 1024\n",
    "            if f.endswith('.tsv'):\n",
    "                row_count = len(pd.read_csv(fp, sep='\\t'))\n",
    "                print(f'  {f}: {row_count} rows ({size_kb:.1f} KB)')\n",
    "            else:\n",
    "                print(f'  {f}: {size_kb:.1f} KB')\n",
    "    else:\n",
    "        print(f'\\n{label}: NOT FOUND')\n",
    "\n",
    "# SQLite summary\n",
    "db_path = os.path.join(pipeline_dir, 'berdl_tables.db')\n",
    "if os.path.exists(db_path):\n",
    "    print(f'\\n{\"=\" * 60}')\n",
    "    print('SQLite Database Tables:')\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    for row in cursor.fetchall():\n",
    "        cursor.execute(f'SELECT COUNT(*) FROM [{row[0]}]')\n",
    "        print(f'  {row[0]}: {cursor.fetchone()[0]} rows')\n",
    "    conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}